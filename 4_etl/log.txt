root
 |-- _corrupt_record: string (nullable = true)

Traceback (most recent call last):
  File "D:/4_PROOF_CONCEPT/3_MAESTRIA/maestria/seminario_unir_001/4_etl/etl2.py", line 21, in <module>
    df.show(20)
  File "D:\bigdata\spark-3.1.2-bin-hadoop3.2\python\lib\pyspark.zip\pyspark\sql\dataframe.py", line 484, in show
  File "D:\bigdata\spark-3.1.2-bin-hadoop3.2\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1305, in __call__
  File "D:\bigdata\spark-3.1.2-bin-hadoop3.2\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
(named _corrupt_record by default). For example:
spark.read.schema(schema).json(file).filter($"_corrupt_record".isNotNull).count()
and spark.read.schema(schema).json(file).select("_corrupt_record").show().
Instead, you can cache or save the parsed results and then send the same query.
For example, val df = spark.read.schema(schema).json(file).cache() and then
df.filter($"_corrupt_record".isNotNull).count().
